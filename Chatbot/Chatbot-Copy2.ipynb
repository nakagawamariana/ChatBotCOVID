{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# things we need for NLP\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# things we need for Tensorflow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "\n",
    "import keras\n",
    "from numpy import loadtxt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# things we need for the conversation \n",
    "import emoji\n",
    "from termcolor import colored\n",
    "from gingerit.gingerit import GingerIt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import training data\n",
    "with open('new_intent.json', 'r') as f:\n",
    "    intents = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Autonomous community telephone numbers\n",
    "with open('comunidades_autonomas.json') as o:\n",
    "    ca = json.load(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. NLP: preprocessing techniques\n",
    "\n",
    "Now we will apply some preprocessing techniques to our training words. The set of all words in each of the patterns from every different tag constitute the training data. \n",
    "\n",
    "Since the user interacting with the chatbot is likely to introduce words different from and in a different sequence than those of our training data, we must select the most relevant words in the most identifiable format to compare them with the user input and be able to find a suitable response. \n",
    "\n",
    "#### 1.1 Tokenization\n",
    "\n",
    "Tokenization is the process of splitting the given text into smaller pieces called tokens. Words, numbers, puntuation marks, and others can be considered as tokens\n",
    "\n",
    "#### 1.2 Stemming\n",
    "Stemming is a process of reducing words to their word stem, base or root form (for example, books = book, looked = look).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words model\n",
    "\n",
    "We will first define our vocabulary by preprocessing the training data with the methods described above and then, we will create vectors with 0's and 1's for each of the sentences with a length equal to the length of the vocabulary, where 0 will mean that the word is absent in the sentence and 1 means that the word is present in the sentence. \n",
    "\n",
    "#### 1. Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize (sentence):\n",
    "    return nltk.word_tokenize(sentence)\n",
    "\n",
    "def stem(word):\n",
    "    return stemmer.stem(word.lower())\n",
    "\n",
    "def bag_of_words (tokenized_sentence, vocabulary):\n",
    "    \n",
    "    \"\"\"\n",
    "    sentence = [\"hello\", \"how\", \"are\", \"you\"]\n",
    "    vocabulary = [\"hi\", \"hello\", \"I\", \"you\", \"bye\", \"thank\", \"cool\"]\n",
    "    bag        = [  0,     1,     0,    1,     0,      0,       0  ]\n",
    "    \n",
    "    \"\"\"\n",
    "    #Stem the sentence\n",
    "    sentence = [stem(w) for w in tokenized_sentence]\n",
    "    \n",
    "    #All-zeros array for bag\n",
    "    bag = np.zeros(len(vocabulary))\n",
    "    \n",
    "    '''\n",
    "    For each word in the vocabulary, if that word is present in the sentence,\n",
    "    put a 1 value in that position correspondant to the position of the word\n",
    "    in the vocabulary list, 0 otherwise. \n",
    "    '''\n",
    "    for idx, w in enumerate(vocabulary):\n",
    "        if w in sentence:\n",
    "            bag[idx] = 1\n",
    "    \n",
    "    return bag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_nostem(tokenized_sentence, vocabulary):\n",
    "    \n",
    "    tokenized_sentence = [w.lower() for w in tokenized_sentence]\n",
    "    \n",
    "    bag = np.zeros(len(vocabulary))\n",
    "    \n",
    "    for idx, w in enumerate(vocabulary):\n",
    "        if w in tokenized_sentence:\n",
    "            bag[idx] = 1\n",
    "    \n",
    "    return bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an empty list with pattern words \n",
    "vocabulary = []\n",
    "#Create an empty list with tag words \n",
    "tags = []\n",
    "#Create an empty list with tag and pattern words \n",
    "xy = []\n",
    "#Create an empty list with tag and tag index \n",
    "tag_idx = []\n",
    "#Create list with responses\n",
    "responses = []\n",
    "\n",
    "for intent in intents['intents']:\n",
    "    tag = intent['tag']\n",
    "    tags.append(tag)\n",
    "    response_list = []\n",
    "    \n",
    "    for pattern in intent['patterns']:\n",
    "        #Tokenize each word in the sentence\n",
    "        words = tokenize(pattern)        \n",
    "        #Add words to the pattern_words list\n",
    "        vocabulary.extend(words)\n",
    "        #Add words and tag tuple to our xy list\n",
    "        xy.append((words,tag))\n",
    "    \n",
    "    for response in intent['responses']:\n",
    "        response_list.append(response)\n",
    "    responses.append(response_list)\n",
    "        \n",
    "#Add tag and tag tuple to our tag_idx list\n",
    "for idx in range(len(tags)):\n",
    "    tag_idx.append((idx, tags[idx]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove punctuation characters and transform to lower case\n",
    "vocabulary = [stem(w.lower()) for w in vocabulary if w not in string.punctuation]\n",
    "\n",
    "#Remove duplicated values\n",
    "vocabulary = sorted(set(vocabulary))\n",
    "tags = sorted(set(tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comunidades aut√≥nomas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an empty list with pattern words \n",
    "vCA = []\n",
    "#Create an empty list with tag words \n",
    "tagsCA = []\n",
    "#Create an empty list with tag and pattern words \n",
    "xyCA = []\n",
    "#Create an empty list with tag and tag index \n",
    "tag_idxCA = []\n",
    "#Create empty list with telephones\n",
    "tlf = []\n",
    "\n",
    "for cam in ca['CA']:\n",
    "    tag = cam['coma']\n",
    "    tagsCA.append(tag)\n",
    "    \n",
    "    for pattern in cam['patterns']:\n",
    "        #Tokenize each word in the sentence\n",
    "        words = tokenize(pattern)        \n",
    "        #Add words to the pattern_words list\n",
    "        vCA.extend(words)\n",
    "        #Add words and tag tuple to our xy list\n",
    "        xyCA.append((words,tag))\n",
    "    \n",
    "    for telephone in cam['telephone']:\n",
    "        tlf.append((cam[\"coma\"], telephone))\n",
    "        \n",
    "        \n",
    "#Add tag and tag tuple to our tag_idx list\n",
    "for idx in range(len(tagsCA)):\n",
    "    tag_idxCA.append((idx, tagsCA[idx]))\n",
    "    \n",
    "#Remove punctuation characters and transform to lower case\n",
    "vCA = [w.lower() for w in vCA if w not in string.punctuation]\n",
    "\n",
    "#Remove duplicated values\n",
    "vCA = sorted(set(vCA))\n",
    "tagsCA = sorted(set(tagsCA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create empty lists for our training data \n",
    "X_trainCA = []\n",
    "y_trainCA = []\n",
    "\n",
    "\"\"\"\n",
    "For each pattern sentence, we will create a bag of words and append it\n",
    "to the X_train dataset, then we will append the output label for that \n",
    "sentence to the y_train dataset. This way, we will create a our training\n",
    "dataset. \n",
    "\n",
    "The X_train dataset will consist of a matrix containing the bag vector of\n",
    "each pattern sentence of 0's and 1's. On the other hand, the y_train will\n",
    "consist of the output labels in the form of indexes, so the indexes will \n",
    "range from 0 to the length of the list \"tags\". \n",
    "\n",
    "\"\"\"\n",
    "for (pattern_sentence, tag) in xyCA:\n",
    "    bag = bag_nostem(pattern_sentence, vCA)\n",
    "    X_trainCA.append(bag)\n",
    "    \n",
    "    label = tagsCA.index(tag)\n",
    "    y_trainCA.append(label)\n",
    "    \n",
    "X_trainCA = np.array(X_trainCA)\n",
    "y_trainCA = np.array(y_trainCA)\n",
    "\n",
    "y_arrayCA = np.zeros((X_trainCA.shape[0], len(tagsCA)))\n",
    "\n",
    "count = 0\n",
    "\n",
    "for idx in range(len(y_trainCA)):\n",
    "    label = y_trainCA[idx]\n",
    "    y_arrayCA[count][label] = 1\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network\n",
    "modelCA = Sequential()\n",
    "modelCA.add(Dense(16, input_dim = X_trainCA.shape[1], activation='relu'))\n",
    "modelCA.add(Dense(12, activation='relu'))\n",
    "modelCA.add(Dense(y_arrayCA.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "37/37 [==============================] - 1s 20ms/step - loss: 2.9440 - accuracy: 0.0270\n",
      "Epoch 2/200\n",
      "37/37 [==============================] - 0s 315us/step - loss: 2.9348 - accuracy: 0.0270\n",
      "Epoch 3/200\n",
      "37/37 [==============================] - 0s 311us/step - loss: 2.9274 - accuracy: 0.0270\n",
      "Epoch 4/200\n",
      "37/37 [==============================] - 0s 333us/step - loss: 2.9207 - accuracy: 0.0541\n",
      "Epoch 5/200\n",
      "37/37 [==============================] - 0s 361us/step - loss: 2.9132 - accuracy: 0.0541\n",
      "Epoch 6/200\n",
      "37/37 [==============================] - 0s 373us/step - loss: 2.9063 - accuracy: 0.0541\n",
      "Epoch 7/200\n",
      "37/37 [==============================] - 0s 353us/step - loss: 2.8992 - accuracy: 0.1081\n",
      "Epoch 8/200\n",
      "37/37 [==============================] - 0s 325us/step - loss: 2.8919 - accuracy: 0.1081\n",
      "Epoch 9/200\n",
      "37/37 [==============================] - 0s 308us/step - loss: 2.8844 - accuracy: 0.0811\n",
      "Epoch 10/200\n",
      "37/37 [==============================] - 0s 321us/step - loss: 2.8765 - accuracy: 0.1081\n",
      "Epoch 11/200\n",
      "37/37 [==============================] - 0s 404us/step - loss: 2.8695 - accuracy: 0.1351\n",
      "Epoch 12/200\n",
      "37/37 [==============================] - 0s 330us/step - loss: 2.8615 - accuracy: 0.1892\n",
      "Epoch 13/200\n",
      "37/37 [==============================] - 0s 348us/step - loss: 2.8535 - accuracy: 0.1622\n",
      "Epoch 14/200\n",
      "37/37 [==============================] - 0s 367us/step - loss: 2.8450 - accuracy: 0.1622\n",
      "Epoch 15/200\n",
      "37/37 [==============================] - 0s 306us/step - loss: 2.8361 - accuracy: 0.2162\n",
      "Epoch 16/200\n",
      "37/37 [==============================] - 0s 314us/step - loss: 2.8276 - accuracy: 0.2162\n",
      "Epoch 17/200\n",
      "37/37 [==============================] - 0s 409us/step - loss: 2.8184 - accuracy: 0.2162\n",
      "Epoch 18/200\n",
      "37/37 [==============================] - 0s 297us/step - loss: 2.8077 - accuracy: 0.2162\n",
      "Epoch 19/200\n",
      "37/37 [==============================] - 0s 400us/step - loss: 2.7979 - accuracy: 0.2162\n",
      "Epoch 20/200\n",
      "37/37 [==============================] - 0s 281us/step - loss: 2.7871 - accuracy: 0.2432\n",
      "Epoch 21/200\n",
      "37/37 [==============================] - 0s 345us/step - loss: 2.7770 - accuracy: 0.2703\n",
      "Epoch 22/200\n",
      "37/37 [==============================] - 0s 310us/step - loss: 2.7660 - accuracy: 0.2703\n",
      "Epoch 23/200\n",
      "37/37 [==============================] - 0s 379us/step - loss: 2.7540 - accuracy: 0.2703\n",
      "Epoch 24/200\n",
      "37/37 [==============================] - 0s 399us/step - loss: 2.7417 - accuracy: 0.2703\n",
      "Epoch 25/200\n",
      "37/37 [==============================] - 0s 345us/step - loss: 2.7294 - accuracy: 0.2703\n",
      "Epoch 26/200\n",
      "37/37 [==============================] - 0s 386us/step - loss: 2.7157 - accuracy: 0.2703\n",
      "Epoch 27/200\n",
      "37/37 [==============================] - 0s 328us/step - loss: 2.7013 - accuracy: 0.2703\n",
      "Epoch 28/200\n",
      "37/37 [==============================] - 0s 289us/step - loss: 2.6881 - accuracy: 0.2703\n",
      "Epoch 29/200\n",
      "37/37 [==============================] - 0s 380us/step - loss: 2.6735 - accuracy: 0.2703\n",
      "Epoch 30/200\n",
      "37/37 [==============================] - 0s 317us/step - loss: 2.6588 - accuracy: 0.2703\n",
      "Epoch 31/200\n",
      "37/37 [==============================] - 0s 390us/step - loss: 2.6444 - accuracy: 0.2703\n",
      "Epoch 32/200\n",
      "37/37 [==============================] - 0s 369us/step - loss: 2.6274 - accuracy: 0.2703\n",
      "Epoch 33/200\n",
      "37/37 [==============================] - 0s 344us/step - loss: 2.6113 - accuracy: 0.2973\n",
      "Epoch 34/200\n",
      "37/37 [==============================] - 0s 334us/step - loss: 2.5961 - accuracy: 0.3243\n",
      "Epoch 35/200\n",
      "37/37 [==============================] - 0s 374us/step - loss: 2.5779 - accuracy: 0.3243\n",
      "Epoch 36/200\n",
      "37/37 [==============================] - 0s 341us/step - loss: 2.5604 - accuracy: 0.3243\n",
      "Epoch 37/200\n",
      "37/37 [==============================] - 0s 298us/step - loss: 2.5432 - accuracy: 0.3243\n",
      "Epoch 38/200\n",
      "37/37 [==============================] - 0s 379us/step - loss: 2.5256 - accuracy: 0.3243\n",
      "Epoch 39/200\n",
      "37/37 [==============================] - 0s 427us/step - loss: 2.5055 - accuracy: 0.3243\n",
      "Epoch 40/200\n",
      "37/37 [==============================] - 0s 391us/step - loss: 2.4867 - accuracy: 0.3243\n",
      "Epoch 41/200\n",
      "37/37 [==============================] - 0s 411us/step - loss: 2.4683 - accuracy: 0.3243\n",
      "Epoch 42/200\n",
      "37/37 [==============================] - 0s 361us/step - loss: 2.4468 - accuracy: 0.3243\n",
      "Epoch 43/200\n",
      "37/37 [==============================] - 0s 362us/step - loss: 2.4272 - accuracy: 0.3784\n",
      "Epoch 44/200\n",
      "37/37 [==============================] - 0s 488us/step - loss: 2.4058 - accuracy: 0.3784\n",
      "Epoch 45/200\n",
      "37/37 [==============================] - 0s 334us/step - loss: 2.3843 - accuracy: 0.4054\n",
      "Epoch 46/200\n",
      "37/37 [==============================] - 0s 326us/step - loss: 2.3636 - accuracy: 0.4054\n",
      "Epoch 47/200\n",
      "37/37 [==============================] - 0s 346us/step - loss: 2.3402 - accuracy: 0.4054\n",
      "Epoch 48/200\n",
      "37/37 [==============================] - 0s 341us/step - loss: 2.3197 - accuracy: 0.3784\n",
      "Epoch 49/200\n",
      "37/37 [==============================] - 0s 385us/step - loss: 2.2969 - accuracy: 0.3784\n",
      "Epoch 50/200\n",
      "37/37 [==============================] - 0s 316us/step - loss: 2.2743 - accuracy: 0.3784\n",
      "Epoch 51/200\n",
      "37/37 [==============================] - 0s 292us/step - loss: 2.2517 - accuracy: 0.3784\n",
      "Epoch 52/200\n",
      "37/37 [==============================] - 0s 327us/step - loss: 2.2287 - accuracy: 0.3784\n",
      "Epoch 53/200\n",
      "37/37 [==============================] - 0s 391us/step - loss: 2.2037 - accuracy: 0.3784\n",
      "Epoch 54/200\n",
      "37/37 [==============================] - 0s 353us/step - loss: 2.1820 - accuracy: 0.4054\n",
      "Epoch 55/200\n",
      "37/37 [==============================] - 0s 292us/step - loss: 2.1581 - accuracy: 0.4595\n",
      "Epoch 56/200\n",
      "37/37 [==============================] - 0s 338us/step - loss: 2.1349 - accuracy: 0.4595\n",
      "Epoch 57/200\n",
      "37/37 [==============================] - 0s 284us/step - loss: 2.1096 - accuracy: 0.4595\n",
      "Epoch 58/200\n",
      "37/37 [==============================] - 0s 306us/step - loss: 2.0862 - accuracy: 0.4595\n",
      "Epoch 59/200\n",
      "37/37 [==============================] - 0s 348us/step - loss: 2.0639 - accuracy: 0.4595\n",
      "Epoch 60/200\n",
      "37/37 [==============================] - 0s 330us/step - loss: 2.0395 - accuracy: 0.4595\n",
      "Epoch 61/200\n",
      "37/37 [==============================] - 0s 356us/step - loss: 2.0146 - accuracy: 0.4595\n",
      "Epoch 62/200\n",
      "37/37 [==============================] - 0s 318us/step - loss: 1.9914 - accuracy: 0.4595\n",
      "Epoch 63/200\n",
      "37/37 [==============================] - 0s 284us/step - loss: 1.9673 - accuracy: 0.4595\n",
      "Epoch 64/200\n",
      "37/37 [==============================] - 0s 304us/step - loss: 1.9430 - accuracy: 0.4595\n",
      "Epoch 65/200\n",
      "37/37 [==============================] - 0s 326us/step - loss: 1.9196 - accuracy: 0.4595\n",
      "Epoch 66/200\n",
      "37/37 [==============================] - 0s 349us/step - loss: 1.8955 - accuracy: 0.4595\n",
      "Epoch 67/200\n",
      "37/37 [==============================] - 0s 371us/step - loss: 1.8719 - accuracy: 0.4595\n",
      "Epoch 68/200\n",
      "37/37 [==============================] - 0s 338us/step - loss: 1.8484 - accuracy: 0.4865\n",
      "Epoch 69/200\n",
      "37/37 [==============================] - 0s 391us/step - loss: 1.8243 - accuracy: 0.4865\n",
      "Epoch 70/200\n",
      "37/37 [==============================] - 0s 298us/step - loss: 1.8018 - accuracy: 0.4865\n",
      "Epoch 71/200\n",
      "37/37 [==============================] - 0s 298us/step - loss: 1.7774 - accuracy: 0.4865\n",
      "Epoch 72/200\n",
      "37/37 [==============================] - 0s 318us/step - loss: 1.7542 - accuracy: 0.4865\n",
      "Epoch 73/200\n",
      "37/37 [==============================] - 0s 303us/step - loss: 1.7318 - accuracy: 0.4865\n",
      "Epoch 74/200\n",
      "37/37 [==============================] - 0s 280us/step - loss: 1.7074 - accuracy: 0.4865\n",
      "Epoch 75/200\n",
      "37/37 [==============================] - 0s 304us/step - loss: 1.6849 - accuracy: 0.5135\n",
      "Epoch 76/200\n",
      "37/37 [==============================] - 0s 343us/step - loss: 1.6615 - accuracy: 0.5135\n",
      "Epoch 77/200\n",
      "37/37 [==============================] - 0s 318us/step - loss: 1.6384 - accuracy: 0.5135\n",
      "Epoch 78/200\n",
      "37/37 [==============================] - 0s 293us/step - loss: 1.6149 - accuracy: 0.5135\n",
      "Epoch 79/200\n",
      "37/37 [==============================] - 0s 330us/step - loss: 1.5927 - accuracy: 0.5135\n",
      "Epoch 80/200\n",
      "37/37 [==============================] - 0s 335us/step - loss: 1.5699 - accuracy: 0.5405\n",
      "Epoch 81/200\n",
      "37/37 [==============================] - 0s 306us/step - loss: 1.5463 - accuracy: 0.5405\n",
      "Epoch 82/200\n",
      "37/37 [==============================] - 0s 300us/step - loss: 1.5235 - accuracy: 0.5405\n",
      "Epoch 83/200\n",
      "37/37 [==============================] - 0s 329us/step - loss: 1.5006 - accuracy: 0.5405\n",
      "Epoch 84/200\n",
      "37/37 [==============================] - 0s 331us/step - loss: 1.4788 - accuracy: 0.5405\n",
      "Epoch 85/200\n",
      "37/37 [==============================] - 0s 334us/step - loss: 1.4572 - accuracy: 0.5405\n",
      "Epoch 86/200\n",
      "37/37 [==============================] - 0s 348us/step - loss: 1.4346 - accuracy: 0.5676\n",
      "Epoch 87/200\n",
      "37/37 [==============================] - 0s 356us/step - loss: 1.4125 - accuracy: 0.5676\n",
      "Epoch 88/200\n",
      "37/37 [==============================] - 0s 329us/step - loss: 1.3906 - accuracy: 0.5946\n",
      "Epoch 89/200\n",
      "37/37 [==============================] - 0s 325us/step - loss: 1.3691 - accuracy: 0.6216\n",
      "Epoch 90/200\n",
      "37/37 [==============================] - 0s 294us/step - loss: 1.3467 - accuracy: 0.6486\n",
      "Epoch 91/200\n",
      "37/37 [==============================] - 0s 313us/step - loss: 1.3254 - accuracy: 0.6486\n",
      "Epoch 92/200\n",
      "37/37 [==============================] - 0s 284us/step - loss: 1.3052 - accuracy: 0.6757\n",
      "Epoch 93/200\n",
      "37/37 [==============================] - 0s 291us/step - loss: 1.2817 - accuracy: 0.6757\n",
      "Epoch 94/200\n",
      "37/37 [==============================] - 0s 303us/step - loss: 1.2609 - accuracy: 0.7027\n",
      "Epoch 95/200\n",
      "37/37 [==============================] - 0s 300us/step - loss: 1.2401 - accuracy: 0.7027\n",
      "Epoch 96/200\n",
      "37/37 [==============================] - 0s 296us/step - loss: 1.2183 - accuracy: 0.7297\n",
      "Epoch 97/200\n",
      "37/37 [==============================] - 0s 284us/step - loss: 1.1989 - accuracy: 0.7297\n",
      "Epoch 98/200\n",
      "37/37 [==============================] - 0s 274us/step - loss: 1.1769 - accuracy: 0.7568\n",
      "Epoch 99/200\n",
      "37/37 [==============================] - 0s 284us/step - loss: 1.1573 - accuracy: 0.7568\n",
      "Epoch 100/200\n",
      "37/37 [==============================] - 0s 294us/step - loss: 1.1370 - accuracy: 0.7838\n",
      "Epoch 101/200\n",
      "37/37 [==============================] - 0s 314us/step - loss: 1.1160 - accuracy: 0.7838\n",
      "Epoch 102/200\n",
      "37/37 [==============================] - 0s 280us/step - loss: 1.0964 - accuracy: 0.7838\n",
      "Epoch 103/200\n",
      "37/37 [==============================] - 0s 311us/step - loss: 1.0771 - accuracy: 0.7838\n",
      "Epoch 104/200\n",
      "37/37 [==============================] - 0s 349us/step - loss: 1.0561 - accuracy: 0.7838\n",
      "Epoch 105/200\n",
      "37/37 [==============================] - 0s 337us/step - loss: 1.0370 - accuracy: 0.7838\n",
      "Epoch 106/200\n",
      "37/37 [==============================] - 0s 316us/step - loss: 1.0167 - accuracy: 0.8108\n",
      "Epoch 107/200\n",
      "37/37 [==============================] - 0s 351us/step - loss: 0.9973 - accuracy: 0.8378\n",
      "Epoch 108/200\n",
      "37/37 [==============================] - 0s 360us/step - loss: 0.9780 - accuracy: 0.8378\n",
      "Epoch 109/200\n",
      "37/37 [==============================] - 0s 328us/step - loss: 0.9590 - accuracy: 0.8378\n",
      "Epoch 110/200\n",
      "37/37 [==============================] - 0s 321us/step - loss: 0.9396 - accuracy: 0.8378\n",
      "Epoch 111/200\n",
      "37/37 [==============================] - 0s 289us/step - loss: 0.9215 - accuracy: 0.8378\n",
      "Epoch 112/200\n",
      "37/37 [==============================] - 0s 300us/step - loss: 0.9030 - accuracy: 0.8378\n",
      "Epoch 113/200\n",
      "37/37 [==============================] - 0s 315us/step - loss: 0.8841 - accuracy: 0.8378\n",
      "Epoch 114/200\n",
      "37/37 [==============================] - 0s 310us/step - loss: 0.8658 - accuracy: 0.8378\n",
      "Epoch 115/200\n",
      "37/37 [==============================] - 0s 334us/step - loss: 0.8480 - accuracy: 0.8378\n",
      "Epoch 116/200\n",
      "37/37 [==============================] - 0s 320us/step - loss: 0.8300 - accuracy: 0.8378\n",
      "Epoch 117/200\n",
      "37/37 [==============================] - 0s 327us/step - loss: 0.8123 - accuracy: 0.8649\n",
      "Epoch 118/200\n",
      "37/37 [==============================] - 0s 308us/step - loss: 0.7937 - accuracy: 0.8649\n",
      "Epoch 119/200\n",
      "37/37 [==============================] - 0s 291us/step - loss: 0.7772 - accuracy: 0.8919\n",
      "Epoch 120/200\n",
      "37/37 [==============================] - 0s 317us/step - loss: 0.7594 - accuracy: 0.8919\n",
      "Epoch 121/200\n",
      "37/37 [==============================] - 0s 291us/step - loss: 0.7434 - accuracy: 0.8919\n",
      "Epoch 122/200\n",
      "37/37 [==============================] - 0s 318us/step - loss: 0.7248 - accuracy: 0.9189\n",
      "Epoch 123/200\n",
      "37/37 [==============================] - 0s 310us/step - loss: 0.7073 - accuracy: 0.9189\n",
      "Epoch 124/200\n",
      "37/37 [==============================] - 0s 291us/step - loss: 0.6908 - accuracy: 0.9459\n",
      "Epoch 125/200\n",
      "37/37 [==============================] - 0s 280us/step - loss: 0.6744 - accuracy: 0.9459\n",
      "Epoch 126/200\n",
      "37/37 [==============================] - 0s 289us/step - loss: 0.6586 - accuracy: 0.9459\n",
      "Epoch 127/200\n",
      "37/37 [==============================] - 0s 281us/step - loss: 0.6426 - accuracy: 0.9459\n",
      "Epoch 128/200\n",
      "37/37 [==============================] - 0s 291us/step - loss: 0.6273 - accuracy: 0.9459\n",
      "Epoch 129/200\n",
      "37/37 [==============================] - 0s 281us/step - loss: 0.6123 - accuracy: 0.9459\n",
      "Epoch 130/200\n",
      "37/37 [==============================] - 0s 331us/step - loss: 0.5968 - accuracy: 0.9730\n",
      "Epoch 131/200\n",
      "37/37 [==============================] - 0s 309us/step - loss: 0.5821 - accuracy: 0.9730\n",
      "Epoch 132/200\n",
      "37/37 [==============================] - 0s 286us/step - loss: 0.5679 - accuracy: 0.9730\n",
      "Epoch 133/200\n",
      "37/37 [==============================] - 0s 291us/step - loss: 0.5537 - accuracy: 0.9730\n",
      "Epoch 134/200\n",
      "37/37 [==============================] - 0s 304us/step - loss: 0.5395 - accuracy: 0.9730\n",
      "Epoch 135/200\n",
      "37/37 [==============================] - 0s 322us/step - loss: 0.5262 - accuracy: 0.9730\n",
      "Epoch 136/200\n",
      "37/37 [==============================] - 0s 299us/step - loss: 0.5144 - accuracy: 0.9730\n",
      "Epoch 137/200\n",
      "37/37 [==============================] - 0s 311us/step - loss: 0.5009 - accuracy: 0.9730\n",
      "Epoch 138/200\n",
      "37/37 [==============================] - 0s 295us/step - loss: 0.4888 - accuracy: 0.9730\n",
      "Epoch 139/200\n",
      "37/37 [==============================] - 0s 303us/step - loss: 0.4758 - accuracy: 0.9730\n",
      "Epoch 140/200\n",
      "37/37 [==============================] - 0s 310us/step - loss: 0.4637 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "37/37 [==============================] - 0s 314us/step - loss: 0.4523 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "37/37 [==============================] - 0s 293us/step - loss: 0.4409 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "37/37 [==============================] - 0s 309us/step - loss: 0.4302 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "37/37 [==============================] - 0s 295us/step - loss: 0.4181 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "37/37 [==============================] - 0s 317us/step - loss: 0.4069 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "37/37 [==============================] - 0s 305us/step - loss: 0.3965 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "37/37 [==============================] - 0s 310us/step - loss: 0.3862 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "37/37 [==============================] - 0s 310us/step - loss: 0.3773 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "37/37 [==============================] - 0s 295us/step - loss: 0.3665 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "37/37 [==============================] - 0s 332us/step - loss: 0.3572 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "37/37 [==============================] - 0s 316us/step - loss: 0.3479 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "37/37 [==============================] - 0s 310us/step - loss: 0.3391 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "37/37 [==============================] - 0s 289us/step - loss: 0.3299 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "37/37 [==============================] - 0s 308us/step - loss: 0.3218 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "37/37 [==============================] - 0s 304us/step - loss: 0.3136 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "37/37 [==============================] - 0s 303us/step - loss: 0.3054 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "37/37 [==============================] - 0s 288us/step - loss: 0.2974 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "37/37 [==============================] - 0s 280us/step - loss: 0.2901 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "37/37 [==============================] - 0s 337us/step - loss: 0.2828 - accuracy: 1.0000\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 313us/step - loss: 0.2754 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "37/37 [==============================] - 0s 279us/step - loss: 0.2683 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "37/37 [==============================] - 0s 305us/step - loss: 0.2616 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "37/37 [==============================] - 0s 322us/step - loss: 0.2548 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "37/37 [==============================] - 0s 315us/step - loss: 0.2488 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "37/37 [==============================] - 0s 310us/step - loss: 0.2423 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "37/37 [==============================] - 0s 308us/step - loss: 0.2359 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "37/37 [==============================] - 0s 303us/step - loss: 0.2304 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "37/37 [==============================] - 0s 315us/step - loss: 0.2245 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "37/37 [==============================] - 0s 302us/step - loss: 0.2189 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "37/37 [==============================] - 0s 299us/step - loss: 0.2134 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "37/37 [==============================] - 0s 303us/step - loss: 0.2086 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "37/37 [==============================] - 0s 315us/step - loss: 0.2038 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "37/37 [==============================] - 0s 302us/step - loss: 0.1986 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "37/37 [==============================] - 0s 298us/step - loss: 0.1937 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "37/37 [==============================] - 0s 327us/step - loss: 0.1890 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "37/37 [==============================] - 0s 321us/step - loss: 0.1851 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "37/37 [==============================] - 0s 312us/step - loss: 0.1806 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "37/37 [==============================] - 0s 300us/step - loss: 0.1765 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "37/37 [==============================] - 0s 315us/step - loss: 0.1725 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "37/37 [==============================] - 0s 296us/step - loss: 0.1685 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "37/37 [==============================] - 0s 295us/step - loss: 0.1648 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "37/37 [==============================] - 0s 299us/step - loss: 0.1609 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "37/37 [==============================] - 0s 291us/step - loss: 0.1577 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "37/37 [==============================] - 0s 308us/step - loss: 0.1537 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "37/37 [==============================] - 0s 304us/step - loss: 0.1507 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "37/37 [==============================] - 0s 287us/step - loss: 0.1471 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "37/37 [==============================] - 0s 329us/step - loss: 0.1441 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "37/37 [==============================] - 0s 287us/step - loss: 0.1408 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "37/37 [==============================] - 0s 299us/step - loss: 0.1377 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "37/37 [==============================] - 0s 308us/step - loss: 0.1346 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "37/37 [==============================] - 0s 304us/step - loss: 0.1317 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "37/37 [==============================] - 0s 331us/step - loss: 0.1294 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "37/37 [==============================] - 0s 311us/step - loss: 0.1262 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "37/37 [==============================] - 0s 319us/step - loss: 0.1234 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "37/37 [==============================] - 0s 308us/step - loss: 0.1212 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "37/37 [==============================] - 0s 304us/step - loss: 0.1185 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "37/37 [==============================] - 0s 316us/step - loss: 0.1163 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "37/37 [==============================] - 0s 310us/step - loss: 0.1138 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "37/37 [==============================] - 0s 290us/step - loss: 0.1114 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "37/37 [==============================] - 0s 286us/step - loss: 0.1093 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7ffa1f2df3d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelCA.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "modelCA.fit(X_trainCA, y_arrayCA, epochs=200, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create vectors for training set\n",
    "\n",
    "This data structure won't work with TensorFlow, which we will use for the neural network model, so we need to transform it further: from sets of words into tensors/arrays of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create empty lists for our training data \n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "\"\"\"\n",
    "For each pattern sentence, we will create a bag of words and append it\n",
    "to the X_train dataset, then we will append the output label for that \n",
    "sentence to the y_train dataset. This way, we will create a our training\n",
    "dataset. \n",
    "\n",
    "The X_train dataset will consist of a matrix containing the bag vector of\n",
    "each pattern sentence of 0's and 1's. On the other hand, the y_train will\n",
    "consist of the output labels in the form of indexes, so the indexes will \n",
    "range from 0 to the length of the list \"tags\". \n",
    "\n",
    "\"\"\"\n",
    "for (pattern_sentence, tag) in xy:\n",
    "    bag = bag_of_words(pattern_sentence, vocabulary)\n",
    "    X_train.append(bag)\n",
    "    \n",
    "    label = tags.index(tag)\n",
    "    y_train.append(label)\n",
    "    \n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "y_array = np.zeros((X_train.shape[0], len(tags)))\n",
    "\n",
    "count = 0\n",
    "\n",
    "for idx in range(len(y_train)):\n",
    "    label = y_train[idx]\n",
    "    y_array[count][label] = 1\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Train the Neural Network Model\n",
    "\n",
    "We will train our model with Keras. Keras is a recommended library for deep learning in Python. Its minimalistic, modular approach makes it a breeze to get deep in neural networks. \n",
    "\n",
    "Deep learning refers to neural networks with multiple hidden layers that can learn increasingly abstract representations of the input data. The higher the number of layers, the more complex the patterns the model learn and the more accurate the classification is. \n",
    "\n",
    "\n",
    "Models with Keras are defined as a sequence of layers. We create a Sequential model and add layers one at a time until we are happy with our network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_TRAIN, X_TEST, Y_TRAIN, Y_TEST = train_test_split(X_train,y_array,test_size = 0.1, random_state = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_TRAIN.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146, 18)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_TRAIN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim = X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(y_array.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 2.8951 - accuracy: 0.0613\n",
      "Epoch 2/200\n",
      "163/163 [==============================] - 0s 283us/step - loss: 2.8627 - accuracy: 0.1350\n",
      "Epoch 3/200\n",
      "163/163 [==============================] - 0s 265us/step - loss: 2.8348 - accuracy: 0.1963\n",
      "Epoch 4/200\n",
      "163/163 [==============================] - 0s 270us/step - loss: 2.8037 - accuracy: 0.2393\n",
      "Epoch 5/200\n",
      "163/163 [==============================] - 0s 262us/step - loss: 2.7666 - accuracy: 0.2577\n",
      "Epoch 6/200\n",
      "163/163 [==============================] - 0s 260us/step - loss: 2.7201 - accuracy: 0.2638\n",
      "Epoch 7/200\n",
      "163/163 [==============================] - 0s 263us/step - loss: 2.6619 - accuracy: 0.2515\n",
      "Epoch 8/200\n",
      "163/163 [==============================] - 0s 266us/step - loss: 2.5920 - accuracy: 0.2209\n",
      "Epoch 9/200\n",
      "163/163 [==============================] - 0s 252us/step - loss: 2.5067 - accuracy: 0.2209\n",
      "Epoch 10/200\n",
      "163/163 [==============================] - 0s 295us/step - loss: 2.4150 - accuracy: 0.2209\n",
      "Epoch 11/200\n",
      "163/163 [==============================] - 0s 271us/step - loss: 2.3226 - accuracy: 0.2209\n",
      "Epoch 12/200\n",
      "163/163 [==============================] - 0s 271us/step - loss: 2.2331 - accuracy: 0.2331\n",
      "Epoch 13/200\n",
      "163/163 [==============================] - 0s 263us/step - loss: 2.1516 - accuracy: 0.2577\n",
      "Epoch 14/200\n",
      "163/163 [==============================] - 0s 267us/step - loss: 2.0709 - accuracy: 0.2883\n",
      "Epoch 15/200\n",
      "163/163 [==============================] - 0s 276us/step - loss: 1.9989 - accuracy: 0.3252\n",
      "Epoch 16/200\n",
      "163/163 [==============================] - 0s 282us/step - loss: 1.9278 - accuracy: 0.3681\n",
      "Epoch 17/200\n",
      "163/163 [==============================] - 0s 252us/step - loss: 1.8607 - accuracy: 0.4417\n",
      "Epoch 18/200\n",
      "163/163 [==============================] - 0s 283us/step - loss: 1.7918 - accuracy: 0.4663\n",
      "Epoch 19/200\n",
      "163/163 [==============================] - 0s 271us/step - loss: 1.7271 - accuracy: 0.4847\n",
      "Epoch 20/200\n",
      "163/163 [==============================] - 0s 264us/step - loss: 1.6594 - accuracy: 0.4908\n",
      "Epoch 21/200\n",
      "163/163 [==============================] - 0s 262us/step - loss: 1.5953 - accuracy: 0.4969\n",
      "Epoch 22/200\n",
      "163/163 [==============================] - 0s 251us/step - loss: 1.5269 - accuracy: 0.5215\n",
      "Epoch 23/200\n",
      "163/163 [==============================] - 0s 254us/step - loss: 1.4626 - accuracy: 0.5399\n",
      "Epoch 24/200\n",
      "163/163 [==============================] - 0s 285us/step - loss: 1.3980 - accuracy: 0.5583\n",
      "Epoch 25/200\n",
      "163/163 [==============================] - 0s 273us/step - loss: 1.3361 - accuracy: 0.5890\n",
      "Epoch 26/200\n",
      "163/163 [==============================] - 0s 288us/step - loss: 1.2764 - accuracy: 0.6258\n",
      "Epoch 27/200\n",
      "163/163 [==============================] - 0s 253us/step - loss: 1.2165 - accuracy: 0.6503\n",
      "Epoch 28/200\n",
      "163/163 [==============================] - 0s 246us/step - loss: 1.1602 - accuracy: 0.6748\n",
      "Epoch 29/200\n",
      "163/163 [==============================] - 0s 287us/step - loss: 1.1081 - accuracy: 0.6810\n",
      "Epoch 30/200\n",
      "163/163 [==============================] - 0s 257us/step - loss: 1.0564 - accuracy: 0.7055\n",
      "Epoch 31/200\n",
      "163/163 [==============================] - 0s 252us/step - loss: 1.0089 - accuracy: 0.7117\n",
      "Epoch 32/200\n",
      "163/163 [==============================] - 0s 286us/step - loss: 0.9643 - accuracy: 0.7362\n",
      "Epoch 33/200\n",
      "163/163 [==============================] - 0s 289us/step - loss: 0.9165 - accuracy: 0.7423\n",
      "Epoch 34/200\n",
      "163/163 [==============================] - 0s 260us/step - loss: 0.8755 - accuracy: 0.7485\n",
      "Epoch 35/200\n",
      "163/163 [==============================] - 0s 252us/step - loss: 0.8323 - accuracy: 0.7669\n",
      "Epoch 36/200\n",
      "163/163 [==============================] - 0s 292us/step - loss: 0.7913 - accuracy: 0.7669\n",
      "Epoch 37/200\n",
      "163/163 [==============================] - 0s 289us/step - loss: 0.7519 - accuracy: 0.7853\n",
      "Epoch 38/200\n",
      "163/163 [==============================] - 0s 277us/step - loss: 0.7157 - accuracy: 0.7975\n",
      "Epoch 39/200\n",
      "163/163 [==============================] - 0s 247us/step - loss: 0.6780 - accuracy: 0.8098\n",
      "Epoch 40/200\n",
      "163/163 [==============================] - 0s 243us/step - loss: 0.6450 - accuracy: 0.8344\n",
      "Epoch 41/200\n",
      "163/163 [==============================] - 0s 246us/step - loss: 0.6129 - accuracy: 0.8344\n",
      "Epoch 42/200\n",
      "163/163 [==============================] - 0s 248us/step - loss: 0.5825 - accuracy: 0.8528\n",
      "Epoch 43/200\n",
      "163/163 [==============================] - 0s 268us/step - loss: 0.5518 - accuracy: 0.8528\n",
      "Epoch 44/200\n",
      "163/163 [==============================] - 0s 267us/step - loss: 0.5235 - accuracy: 0.8712\n",
      "Epoch 45/200\n",
      "163/163 [==============================] - 0s 264us/step - loss: 0.5010 - accuracy: 0.8834\n",
      "Epoch 46/200\n",
      "163/163 [==============================] - 0s 267us/step - loss: 0.4744 - accuracy: 0.9018\n",
      "Epoch 47/200\n",
      "163/163 [==============================] - 0s 267us/step - loss: 0.4504 - accuracy: 0.9018\n",
      "Epoch 48/200\n",
      "163/163 [==============================] - 0s 273us/step - loss: 0.4280 - accuracy: 0.9018\n",
      "Epoch 49/200\n",
      "163/163 [==============================] - 0s 278us/step - loss: 0.4068 - accuracy: 0.9080\n",
      "Epoch 50/200\n",
      "163/163 [==============================] - 0s 274us/step - loss: 0.3865 - accuracy: 0.9202\n",
      "Epoch 51/200\n",
      "163/163 [==============================] - 0s 263us/step - loss: 0.3671 - accuracy: 0.9264\n",
      "Epoch 52/200\n",
      "163/163 [==============================] - 0s 263us/step - loss: 0.3491 - accuracy: 0.9448\n",
      "Epoch 53/200\n",
      "163/163 [==============================] - 0s 263us/step - loss: 0.3310 - accuracy: 0.9509\n",
      "Epoch 54/200\n",
      "163/163 [==============================] - 0s 266us/step - loss: 0.3138 - accuracy: 0.9509\n",
      "Epoch 55/200\n",
      "163/163 [==============================] - 0s 270us/step - loss: 0.2983 - accuracy: 0.9632\n",
      "Epoch 56/200\n",
      "163/163 [==============================] - 0s 280us/step - loss: 0.2836 - accuracy: 0.9571\n",
      "Epoch 57/200\n",
      "163/163 [==============================] - 0s 285us/step - loss: 0.2687 - accuracy: 0.9693\n",
      "Epoch 58/200\n",
      "163/163 [==============================] - 0s 291us/step - loss: 0.2539 - accuracy: 0.9693\n",
      "Epoch 59/200\n",
      "163/163 [==============================] - 0s 283us/step - loss: 0.2409 - accuracy: 0.9693\n",
      "Epoch 60/200\n",
      "163/163 [==============================] - 0s 283us/step - loss: 0.2294 - accuracy: 0.9693\n",
      "Epoch 61/200\n",
      "163/163 [==============================] - 0s 285us/step - loss: 0.2172 - accuracy: 0.9693\n",
      "Epoch 62/200\n",
      "163/163 [==============================] - 0s 279us/step - loss: 0.2068 - accuracy: 0.9755\n",
      "Epoch 63/200\n",
      "163/163 [==============================] - 0s 283us/step - loss: 0.1971 - accuracy: 0.9816\n",
      "Epoch 64/200\n",
      "163/163 [==============================] - 0s 281us/step - loss: 0.1863 - accuracy: 0.9877\n",
      "Epoch 65/200\n",
      "163/163 [==============================] - 0s 285us/step - loss: 0.1759 - accuracy: 0.9939\n",
      "Epoch 66/200\n",
      "163/163 [==============================] - 0s 284us/step - loss: 0.1670 - accuracy: 0.9939\n",
      "Epoch 67/200\n",
      "163/163 [==============================] - 0s 286us/step - loss: 0.1579 - accuracy: 1.0000\n",
      "Epoch 68/200\n",
      "163/163 [==============================] - 0s 279us/step - loss: 0.1492 - accuracy: 1.0000\n",
      "Epoch 69/200\n",
      "163/163 [==============================] - 0s 277us/step - loss: 0.1417 - accuracy: 1.0000\n",
      "Epoch 70/200\n",
      "163/163 [==============================] - 0s 274us/step - loss: 0.1336 - accuracy: 1.0000\n",
      "Epoch 71/200\n",
      "163/163 [==============================] - 0s 283us/step - loss: 0.1273 - accuracy: 1.0000\n",
      "Epoch 72/200\n",
      "163/163 [==============================] - 0s 279us/step - loss: 0.1200 - accuracy: 1.0000\n",
      "Epoch 73/200\n",
      "163/163 [==============================] - 0s 293us/step - loss: 0.1143 - accuracy: 1.0000\n",
      "Epoch 74/200\n",
      "163/163 [==============================] - 0s 285us/step - loss: 0.1084 - accuracy: 1.0000\n",
      "Epoch 75/200\n",
      "163/163 [==============================] - 0s 289us/step - loss: 0.1031 - accuracy: 1.0000\n",
      "Epoch 76/200\n",
      "163/163 [==============================] - 0s 287us/step - loss: 0.0980 - accuracy: 1.0000\n",
      "Epoch 77/200\n",
      "163/163 [==============================] - 0s 270us/step - loss: 0.0936 - accuracy: 1.0000\n",
      "Epoch 78/200\n",
      "163/163 [==============================] - 0s 272us/step - loss: 0.0889 - accuracy: 1.0000\n",
      "Epoch 79/200\n",
      "163/163 [==============================] - 0s 269us/step - loss: 0.0843 - accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "163/163 [==============================] - 0s 275us/step - loss: 0.0802 - accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "163/163 [==============================] - 0s 260us/step - loss: 0.0766 - accuracy: 1.0000\n",
      "Epoch 82/200\n",
      "163/163 [==============================] - 0s 265us/step - loss: 0.0727 - accuracy: 1.0000\n",
      "Epoch 83/200\n",
      "163/163 [==============================] - 0s 258us/step - loss: 0.0698 - accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "163/163 [==============================] - 0s 256us/step - loss: 0.0665 - accuracy: 1.0000\n",
      "Epoch 85/200\n",
      "163/163 [==============================] - 0s 259us/step - loss: 0.0638 - accuracy: 1.0000\n",
      "Epoch 86/200\n",
      "163/163 [==============================] - 0s 255us/step - loss: 0.0609 - accuracy: 1.0000\n",
      "Epoch 87/200\n",
      "163/163 [==============================] - 0s 267us/step - loss: 0.0578 - accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "163/163 [==============================] - 0s 276us/step - loss: 0.0551 - accuracy: 1.0000\n",
      "Epoch 89/200\n",
      "163/163 [==============================] - 0s 280us/step - loss: 0.0525 - accuracy: 1.0000\n",
      "Epoch 90/200\n",
      "163/163 [==============================] - 0s 347us/step - loss: 0.0506 - accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "163/163 [==============================] - 0s 340us/step - loss: 0.0488 - accuracy: 1.0000\n",
      "Epoch 92/200\n",
      "163/163 [==============================] - 0s 326us/step - loss: 0.0466 - accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "163/163 [==============================] - 0s 331us/step - loss: 0.0448 - accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "163/163 [==============================] - 0s 346us/step - loss: 0.0426 - accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "163/163 [==============================] - 0s 334us/step - loss: 0.0411 - accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "163/163 [==============================] - 0s 324us/step - loss: 0.0396 - accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "163/163 [==============================] - 0s 368us/step - loss: 0.0380 - accuracy: 1.0000\n",
      "Epoch 98/200\n",
      "163/163 [==============================] - 0s 345us/step - loss: 0.0366 - accuracy: 1.0000\n",
      "Epoch 99/200\n",
      "163/163 [==============================] - 0s 385us/step - loss: 0.0354 - accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "163/163 [==============================] - 0s 354us/step - loss: 0.0341 - accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "163/163 [==============================] - 0s 340us/step - loss: 0.0329 - accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "163/163 [==============================] - 0s 336us/step - loss: 0.0318 - accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "163/163 [==============================] - 0s 273us/step - loss: 0.0308 - accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "163/163 [==============================] - 0s 268us/step - loss: 0.0297 - accuracy: 1.0000\n",
      "Epoch 105/200\n",
      "163/163 [==============================] - 0s 288us/step - loss: 0.0287 - accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "163/163 [==============================] - 0s 291us/step - loss: 0.0279 - accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "163/163 [==============================] - 0s 284us/step - loss: 0.0269 - accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "163/163 [==============================] - 0s 288us/step - loss: 0.0261 - accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "163/163 [==============================] - 0s 280us/step - loss: 0.0253 - accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "163/163 [==============================] - 0s 272us/step - loss: 0.0243 - accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "163/163 [==============================] - 0s 281us/step - loss: 0.0235 - accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "163/163 [==============================] - 0s 264us/step - loss: 0.0229 - accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "163/163 [==============================] - 0s 272us/step - loss: 0.0222 - accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "163/163 [==============================] - 0s 265us/step - loss: 0.0215 - accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "163/163 [==============================] - 0s 249us/step - loss: 0.0209 - accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "163/163 [==============================] - 0s 268us/step - loss: 0.0203 - accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "163/163 [==============================] - 0s 256us/step - loss: 0.0197 - accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "163/163 [==============================] - 0s 254us/step - loss: 0.0192 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "163/163 [==============================] - 0s 277us/step - loss: 0.0186 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "163/163 [==============================] - 0s 266us/step - loss: 0.0181 - accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "163/163 [==============================] - 0s 263us/step - loss: 0.0176 - accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "163/163 [==============================] - 0s 264us/step - loss: 0.0171 - accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "163/163 [==============================] - 0s 269us/step - loss: 0.0167 - accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "163/163 [==============================] - 0s 277us/step - loss: 0.0162 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "163/163 [==============================] - 0s 275us/step - loss: 0.0157 - accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "163/163 [==============================] - 0s 258us/step - loss: 0.0154 - accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "163/163 [==============================] - 0s 265us/step - loss: 0.0149 - accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "163/163 [==============================] - 0s 264us/step - loss: 0.0145 - accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "163/163 [==============================] - 0s 262us/step - loss: 0.0142 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "163/163 [==============================] - 0s 267us/step - loss: 0.0138 - accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "163/163 [==============================] - 0s 271us/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "163/163 [==============================] - 0s 262us/step - loss: 0.0132 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "163/163 [==============================] - 0s 264us/step - loss: 0.0129 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "163/163 [==============================] - 0s 257us/step - loss: 0.0125 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "163/163 [==============================] - 0s 265us/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "163/163 [==============================] - 0s 267us/step - loss: 0.0120 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "163/163 [==============================] - 0s 254us/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "163/163 [==============================] - 0s 262us/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "163/163 [==============================] - 0s 258us/step - loss: 0.0111 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "163/163 [==============================] - 0s 266us/step - loss: 0.0109 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "163/163 [==============================] - 0s 263us/step - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "163/163 [==============================] - 0s 266us/step - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "163/163 [==============================] - 0s 257us/step - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "163/163 [==============================] - 0s 267us/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "163/163 [==============================] - 0s 269us/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "163/163 [==============================] - 0s 257us/step - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "163/163 [==============================] - 0s 257us/step - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "163/163 [==============================] - 0s 254us/step - loss: 0.0090 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "163/163 [==============================] - 0s 259us/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "163/163 [==============================] - 0s 261us/step - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "163/163 [==============================] - 0s 258us/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "163/163 [==============================] - 0s 255us/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "163/163 [==============================] - 0s 257us/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "163/163 [==============================] - 0s 256us/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "163/163 [==============================] - 0s 264us/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "163/163 [==============================] - 0s 264us/step - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - 0s 262us/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "163/163 [==============================] - 0s 272us/step - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "163/163 [==============================] - 0s 263us/step - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "163/163 [==============================] - 0s 264us/step - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "163/163 [==============================] - 0s 254us/step - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "163/163 [==============================] - 0s 258us/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "163/163 [==============================] - 0s 271us/step - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "163/163 [==============================] - 0s 280us/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "163/163 [==============================] - 0s 280us/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "163/163 [==============================] - 0s 285us/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "163/163 [==============================] - 0s 282us/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "163/163 [==============================] - 0s 276us/step - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "163/163 [==============================] - 0s 279us/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "163/163 [==============================] - 0s 282us/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "163/163 [==============================] - 0s 279us/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "163/163 [==============================] - 0s 284us/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "163/163 [==============================] - 0s 266us/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "163/163 [==============================] - 0s 258us/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "163/163 [==============================] - 0s 279us/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "163/163 [==============================] - 0s 278us/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "163/163 [==============================] - 0s 279us/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "163/163 [==============================] - 0s 266us/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "163/163 [==============================] - 0s 256us/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "163/163 [==============================] - 0s 268us/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "163/163 [==============================] - 0s 259us/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "163/163 [==============================] - 0s 270us/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "163/163 [==============================] - 0s 268us/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "163/163 [==============================] - 0s 252us/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "163/163 [==============================] - 0s 263us/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "163/163 [==============================] - 0s 263us/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "163/163 [==============================] - 0s 261us/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "163/163 [==============================] - 0s 255us/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "163/163 [==============================] - 0s 253us/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "163/163 [==============================] - 0s 250us/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "163/163 [==============================] - 0s 267us/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "163/163 [==============================] - 0s 251us/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "163/163 [==============================] - 0s 265us/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "163/163 [==============================] - 0s 262us/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "163/163 [==============================] - 0s 255us/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "163/163 [==============================] - 0s 248us/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "163/163 [==============================] - 0s 258us/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "163/163 [==============================] - 0s 254us/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "163/163 [==============================] - 0s 266us/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "163/163 [==============================] - 0s 265us/step - loss: 0.0034 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7ffa1ff0d4d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_array, epochs=200, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mHi! My name is Cova\u001b[0m ü§ñ \u001b[1m\n",
      "\n",
      "I have been created to try to help you with all your possible questions related to Covid-19.\n",
      "I am delighted to speak with you and help you in any way I can.\u001b[0m \u001b[1m\n",
      "\n",
      "I can give you information about many things!\u001b[0m \n",
      "\n",
      "‚úÖ \u001b[1mWhat is the coronavirus?\u001b[0m ü¶† \n",
      "‚úÖ \u001b[1mCovid phone\u001b[0m üìû \n",
      "‚úÖ \u001b[1mSymptoms\u001b[0m ü§ßüå° \n",
      "‚úÖ \u001b[1mContact with positives\u001b[0m ü§ù‚ö† \n",
      "‚úÖ \u001b[1mQuarantine\u001b[0m üè† \n",
      "‚úÖ \u001b[1mTips to protect yourself from the virus\u001b[0m üëèüßº‚ú® \u001b[1m\n",
      "\n",
      "Come on! Ask me something\u001b[0m üòÅ\n",
      "\n",
      "covid phone la rioja\n",
      "\n",
      "\u001b[1mIt's important to not to collapse emergency phone lines. If you only need information, you should call the Covid phone number provided by your autonomous community.\n",
      "\n",
      "üÜò‚òé Just in case of emergency, call 112. \u001b[0m\n",
      "\u001b[1m\n",
      "\n",
      "The telephone number of La Rioja is: 941 298 333\u001b[0m\n",
      "\n",
      "covid phone andalucia\n",
      "\n",
      "\u001b[1mIt's important to not to collapse emergency phone lines. If you only need information, you should call the Covid phone number provided by your autonomous community.\n",
      "\n",
      "üÜò‚òé Just in case of emergency, call 112. \u001b[0m\n",
      "\u001b[1m\n",
      "\n",
      "The telephone number of Andalucia is: 900 400 061 / 955 545 060\u001b[0m\n",
      "\n",
      "give me the telephone of la rioja\n",
      "\n",
      "\u001b[1mIt's important to not to collapse emergency phone lines. If you only need information, you should call the Covid phone number provided by your autonomous community.\n",
      "\n",
      "üÜò‚òé Just in case of emergency, call 112. \u001b[0m\n",
      "\u001b[1m\n",
      "\n",
      "The telephone number of La Rioja is: 941 298 333\u001b[0m\n",
      "\n",
      "how are u\n",
      "\n",
      "\u001b[1mI'm well, thanks!\u001b[0m\n",
      "\n",
      "how are you doing\n",
      "\n",
      "\u001b[1mPretty good, thanks!\u001b[0m\n",
      "\n",
      "what is covid quarantine\n",
      "\n",
      "\u001b[1m‚ÑπWhat is quarantine?\n",
      "It is a public health measure to control the transmission of the COVID-19 virus. It is indicated so that people who have been exposed to the virus do not have contact with other people during the period in which they can develop and transmit the infection.\n",
      "\n",
      "üè†What is quarantine about?\n",
      "The recommended measures, as far as possible, are to remain alone in a room of the house for as long as possible, have your own bathroom and wear a surgical mask whenever you need to leave the room. In this case, the cohabitants must also wear a surgical mask. Nor should you leave your home during quarantine.\n",
      "\n",
      "üìÖHow long is the quarantine?\n",
      "10 days from the last contact with the confirmed case. The quarantine is 10 days because most people develop symptoms in the first 10 days after being exposed to a case of COVID-19. Watch for symptoms during the quarantine and also during the 4 days after it ends.\n",
      "\n",
      "‚ùóWhy is so important to keep quarantine?\n",
      "The COVID-19 virus can be transmitted from two days before the onset of symptoms. It can also be transmitted from asymptomatic people. Even if a diagnostic test is performed at the beginning of the quarantine period and it is negative, it is important to continue the quarantine period until the end, since symptoms of the disease may appear after the result of the diagnostic test and throughout the entire period of quarantine.\u001b[0m\n",
      "\n",
      "quarantine\n",
      "\n",
      "\u001b[1m‚ÑπWhat is quarantine?\n",
      "It is a public health measure to control the transmission of the COVID-19 virus. It is indicated so that people who have been exposed to the virus do not have contact with other people during the period in which they can develop and transmit the infection.\n",
      "\n",
      "üè†What is quarantine about?\n",
      "The recommended measures, as far as possible, are to remain alone in a room of the house for as long as possible, have your own bathroom and wear a surgical mask whenever you need to leave the room. In this case, the cohabitants must also wear a surgical mask. Nor should you leave your home during quarantine.\n",
      "\n",
      "üìÖHow long is the quarantine?\n",
      "10 days from the last contact with the confirmed case. The quarantine is 10 days because most people develop symptoms in the first 10 days after being exposed to a case of COVID-19. Watch for symptoms during the quarantine and also during the 4 days after it ends.\n",
      "\n",
      "‚ùóWhy is so important to keep quarantine?\n",
      "The COVID-19 virus can be transmitted from two days before the onset of symptoms. It can also be transmitted from asymptomatic people. Even if a diagnostic test is performed at the beginning of the quarantine period and it is negative, it is important to continue the quarantine period until the end, since symptoms of the disease may appear after the result of the diagnostic test and throughout the entire period of quarantine.\u001b[0m\n",
      "\n",
      "protection measures\n",
      "\n",
      "\u001b[1mü§ùüßº‚ú® Wash your hands frequently.\n",
      "\n",
      "üò∑ Wear a mask.\n",
      "\n",
      "üôÖ Avoid touching your eyes, nose and mouth. When coughing, cover your mouth and nose with your elbow.\n",
      "\n",
      "üßç‚ñ´‚ñ´üßç Keep a distance of, at least, 1.5 m with other people. Limit your contacts.\n",
      "\n",
      "üèû Prioritize open spaces.\n",
      "\n",
      "ü™ü Ventilate closed spaces frequently.\n",
      "\n",
      "üè† Stay at home if you have any symptoms or if you have to quarantine.\u001b[0m\n",
      "\n",
      "what I have to do if I have no symptoms\n",
      "\n",
      "\u001b[1m‚ÑπIt's important to note that Covid-19 affects in different ways depending on each person.\n",
      "\n",
      "It's possible that a person infected with Covid-19 does not present any symptom, or only has mild symptoms. However, this person can spread the virus and infect other people.\n",
      "\n",
      "Moreover, it takes 5-6 days on average from when someone is infected with the virus for symptoms to show. It can even take up to 14 days.\u001b[0m\n",
      "\n",
      "what do to with no symptoms\n",
      "\n",
      "\u001b[1m‚ÑπIt's important to note that Covid-19 affects in different ways depending on each person.\n",
      "\n",
      "It's possible that a person infected with Covid-19 does not present any symptom, or only has mild symptoms. However, this person can spread the virus and infect other people.\n",
      "\n",
      "Moreover, it takes 5-6 days on average from when someone is infected with the virus for symptoms to show. It can even take up to 14 days.\u001b[0m\n",
      "\n",
      "tell me a joke\n",
      "\n",
      "\u001b[1mNothing like relaxing on the couch after a long day of being tense on the couch üõã\u001b[0m\n",
      "\n",
      "what is your name\n",
      "\n",
      "\u001b[1mMy name is Cova\u001b[0m\n",
      "\n",
      "what's your name\n",
      "\n",
      "\u001b[1mMy name is Cova\u001b[0m\n",
      "\n",
      "tell me your name\n",
      "\n",
      "\u001b[1mMy name is Cova\u001b[0m\n",
      "\n",
      "give me information about protection measures\n",
      "\n",
      "\u001b[1mü§ùüßº‚ú® Wash your hands frequently.\n",
      "\n",
      "üò∑ Wear a mask.\n",
      "\n",
      "üôÖ Avoid touching your eyes, nose and mouth. When coughing, cover your mouth and nose with your elbow.\n",
      "\n",
      "üßç‚ñ´‚ñ´üßç Keep a distance of, at least, 1.5 m with other people. Limit your contacts.\n",
      "\n",
      "üèû Prioritize open spaces.\n",
      "\n",
      "ü™ü Ventilate closed spaces frequently.\n",
      "\n",
      "üè† Stay at home if you have any symptoms or if you have to quarantine.\u001b[0m\n",
      "\n",
      "information about protection measures\n",
      "\n",
      "\u001b[1mü§ùüßº‚ú® Wash your hands frequently.\n",
      "\n",
      "üò∑ Wear a mask.\n",
      "\n",
      "üôÖ Avoid touching your eyes, nose and mouth. When coughing, cover your mouth and nose with your elbow.\n",
      "\n",
      "üßç‚ñ´‚ñ´üßç Keep a distance of, at least, 1.5 m with other people. Limit your contacts.\n",
      "\n",
      "üèû Prioritize open spaces.\n",
      "\n",
      "ü™ü Ventilate closed spaces frequently.\n",
      "\n",
      "üè† Stay at home if you have any symptoms or if you have to quarantine.\u001b[0m\n",
      "\n",
      "covid phone\n",
      "\n",
      "\u001b[1mIt's important to not to collapse emergency phone lines. If you only need information, you should call the Covid phone number provided by your autonomous community.\n",
      "\n",
      "üÜò‚òé Just in case of emergency, call 112. \u001b[0m\n",
      "\u001b[1m\n",
      "I can help you with Covid-19 phone numbers. What is your autonomous community?\n",
      "\u001b[0m\n",
      "pais vasco\n",
      "\u001b[1m\n",
      "The telephone number of Pais Vasco is: 900 203 050\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(colored(\"Hi! My name is Cova\", attrs = [\"bold\"]) , \"\\U0001F916\",\n",
    "      colored(\"\\n\\nI have been created to try to help you with all your possible questions related to Covid-19.\\nI am delighted to speak with you and help you in any way I can.\",attrs = [\"bold\"]),\n",
    "      colored(\"\\n\\nI can give you information about many things!\", attrs = [\"bold\"]), \n",
    "      \"\\n\\n\\U00002705\", colored(\"What is the coronavirus?\", attrs = [\"bold\"]), \"\\U0001F9A0\",\n",
    "      \"\\n\\U00002705\", colored(\"Covid phone\", attrs = [\"bold\"]),\"\\U0001F4DE\",\n",
    "      \"\\n\\U00002705\", colored(\"Symptoms\", attrs = [\"bold\"]),\"\\U0001F927\\U0001F321\",      \n",
    "      \"\\n\\U00002705\", colored(\"Contact with positives\", attrs = [\"bold\"]),\"\\U0001F91D\\U000026A0\",\n",
    "      \"\\n\\U00002705\", colored(\"Quarantine\", attrs = [\"bold\"]),\"\\U0001F3E0\", \n",
    "      \"\\n\\U00002705\", colored(\"Tips to protect yourself from the virus\", attrs = [\"bold\"]),\"\\U0001F44F\\U0001F9FC\\U00002728\",\n",
    "      colored(\"\\n\\nCome on! Ask me something\", attrs =[\"bold\"]), \"\\U0001F601\"\n",
    "\n",
    "     )\n",
    "\n",
    "conversation = True\n",
    "\n",
    "\n",
    "while conversation:\n",
    "    \n",
    "    print(\"\")\n",
    "    #Ask for a question from the user\n",
    "    response = input()\n",
    "    \n",
    "    #Response without grammar parser\n",
    "    response_p = tokenize(response)\n",
    "    \n",
    "    #Grammar correction of the question\n",
    "    parser = GingerIt()\n",
    "    result = parser.parse(response)\n",
    "    response = result['result']\n",
    "    \n",
    "    #Tokenization and bag of words\n",
    "    x_sentence = tokenize(response)\n",
    "    bag_x = bag_of_words(x_sentence, vocabulary)\n",
    "    bag_test = np.zeros((1, X_train.shape[1]))\n",
    "    bag_test[0] = bag_x\n",
    "    \n",
    "    #Prediction of the response\n",
    "    predictions = model.predict_classes(bag_test)\n",
    "    probability = model.predict_proba(bag_test)\n",
    "    \n",
    "    #for element in probability[0]:\n",
    "    #    print(element)\n",
    "    \n",
    "    if probability[0][predictions[0]] >= 0.85:\n",
    "    \n",
    "        label_response = tags[predictions[0]]\n",
    "\n",
    "        for idx, element in tag_idx:\n",
    "            \n",
    "            if element == label_response:\n",
    "                print(\"\")\n",
    "                output = colored(random.choice(responses[idx]).encode('ascii').decode('unicode-escape'), attrs = [\"bold\"])\n",
    "                print(output)\n",
    "                \n",
    "                if label_response == 'CovidTelephone':\n",
    "                    \n",
    "                    #Bag of words for Autonomous Community\n",
    "                    bag_ca = bag_nostem(response_p, vCA)\n",
    "                    bag_testca = np.zeros((1, X_trainCA.shape[1]))\n",
    "                    bag_testca[0] = bag_ca\n",
    "                    \n",
    "                    #Predictions for Autonomous Community\n",
    "                    prediction_ca = modelCA.predict_classes(bag_testca)\n",
    "                    probability_ca = modelCA.predict_proba(bag_testca)\n",
    "                    \n",
    "                    if probability_ca[0][prediction_ca][0] >= 0.25:\n",
    "                        community = tagsCA[prediction_ca[0]]\n",
    "                        \n",
    "                        for com, phone in tlf: \n",
    "                            if com == community:\n",
    "                                phone_number = phone\n",
    "                            \n",
    "                        response_tlf = \"\\n\\nThe telephone number of {} is: {}\".format(community, phone_number)\n",
    "                        print(colored(response_tlf, attrs = [\"bold\"]))\n",
    "                    \n",
    "                    else:\n",
    "                        print(colored(\"\\nI can help you with Covid-19 phone numbers. What is your autonomous community?\\n\", attrs=[\"bold\"]))\n",
    "                        \n",
    "                        input_ca = input()\n",
    "\n",
    "                        #Tokenize input and bag of words\n",
    "                        input_ca = tokenize(input_ca)\n",
    "                        bag_caq = bag_nostem(input_ca, vCA)\n",
    "                        bag_testcaq = np.zeros((1, X_trainCA.shape[1]))\n",
    "                        bag_testcaq[0] = bag_caq\n",
    "\n",
    "\n",
    "                        #Predictions for Autonomous Community\n",
    "                        prediction_caq = modelCA.predict_classes(bag_testcaq)\n",
    "                        probability_caq = modelCA.predict_proba(bag_testcaq)\n",
    "                        if probability_caq[0][prediction_caq][0] >= 0.25:\n",
    "                            community = tagsCA[prediction_caq[0]]\n",
    "\n",
    "                            for com, phone in tlf: \n",
    "                                if com == community:\n",
    "                                    phone_number = phone\n",
    "\n",
    "                            response_tlf = \"\\nThe telephone number of {} is: {}\".format(community, phone_number)\n",
    "                            print(colored(response_tlf, attrs = [\"bold\"]))\n",
    "                        \n",
    "                        else:\n",
    "                            print(colored(\"\\nIm sorry, I didn't understand your response\", attrs = [\"bold\"]))\n",
    "    else:\n",
    "        print(colored(\"\\nIm sorry, I didn't understand your question\", attrs = [\"bold\"]))\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
